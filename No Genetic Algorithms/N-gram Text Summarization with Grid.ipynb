{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba2f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\srina\\AppData\\Local\\Temp\\tmphqyxowkv\\config.json as plain json\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "model_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    "predictor = Predictor.from_path(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d362827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rouge\n",
    "ROUGE = rouge.Rouge(metrics=['rouge-n'],\n",
    "                           max_n=1)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5078c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/SummarizationCSTitleAbstract03.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f996963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Microbiological Survey and Characterization</td>\n",
       "      <td>In our study, two dairy compost heaps and one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb 2 00 6 Non-Commutative Formal Groups in Pos...</td>\n",
       "      <td>We describe geometric non-commutative formal g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Alternating Mesh Quality Metric Scheme for ...</td>\n",
       "      <td>In the numerical solution of partial different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Researching Distance Learning Experiences Usin...</td>\n",
       "      <td>Qualitative case study is hardly a research te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un Motor de Transformación de Modelos con Sopo...</td>\n",
       "      <td>Resumen. En la actualidad están apareciendo un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      A Microbiological Survey and Characterization   \n",
       "1  eb 2 00 6 Non-Commutative Formal Groups in Pos...   \n",
       "2  An Alternating Mesh Quality Metric Scheme for ...   \n",
       "3  Researching Distance Learning Experiences Usin...   \n",
       "4  Un Motor de Transformación de Modelos con Sopo...   \n",
       "\n",
       "                                            abstract  \n",
       "0  In our study, two dairy compost heaps and one ...  \n",
       "1  We describe geometric non-commutative formal g...  \n",
       "2  In the numerical solution of partial different...  \n",
       "3  Qualitative case study is hardly a research te...  \n",
       "4  Resumen. En la actualidad están apareciendo un...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df = df.head(100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03edd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "\n",
    "# def resolve(corenlp_output):\n",
    "#     \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
    "#     for coref in corenlp_output['corefs']:\n",
    "#         mentions = corenlp_output['corefs'][coref]\n",
    "#         antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
    "#         for j in range(1, len(mentions)):\n",
    "#             mention = mentions[j]\n",
    "#             if mention['type'] == 'PRONOMINAL':\n",
    "#                 # get the attributes of the target mention in the corresponding sentence\n",
    "#                 target_sentence = mention['sentNum']\n",
    "#                 target_token = mention['startIndex'] - 1\n",
    "#                 # transfer the antecedent's word form to the appropriate token in the sentence\n",
    "#                 corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
    "\n",
    "\n",
    "# def get_resolved(corenlp_output):\n",
    "#     \"\"\"Return the \"resolved\" output sentence\"\"\"\n",
    "#     possessives = ['hers', 'his', 'their', 'theirs']\n",
    "#     output_sentence = \"\"  # Empty string to accumulate the output sentence\n",
    "    \n",
    "#     for sentence in corenlp_output['sentences']:\n",
    "#         for token in sentence['tokens']:\n",
    "#             output_word = token['word']\n",
    "#             if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
    "#                 output_word += \"'s\"\n",
    "#             output_word += token['after']\n",
    "#             output_sentence += output_word  # Append the output_word to the output_sentence\n",
    "    \n",
    "#     return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "577eaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def sentence_segment(self, doc, lower):\n",
    "        sentences = []\n",
    "        pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "        sents = re.split(pattern, doc)\n",
    "        sents = [sentence.split() for sentence in sents if any(char.isalpha() for char in sentence)]\n",
    "        \n",
    "        for sent in sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if lower is True:\n",
    "                    selected_words.append(token.lower())\n",
    "                else:\n",
    "                    selected_words.append(token)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                window_size=4, lower=False):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(text, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a08ff7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(dataframe, weights):\n",
    "    # Filling in Nan values\n",
    "    dataframe['abstract'] = dataframe['abstract'].fillna('This abstract does not exist')\n",
    "\n",
    "    # Getting tf-idf tables\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(dataframe['abstract'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Generating best title for each abstract\n",
    "    summaries = []\n",
    "    scores = []\n",
    "    resolved_abstracts = []\n",
    "    for i in range(len(dataframe)):\n",
    "        abstract = dataframe.loc[i, 'abstract']\n",
    "        #print('\\nOriginal:', abstract)\n",
    "        \n",
    "#         # Stanford Anaphora Resolution\n",
    "#         output = nlp.annotate(abstract, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
    "#         output = json.loads(output)\n",
    "#         resolve(output)\n",
    "#         abstract = get_resolved(output)\n",
    "#         print('\\nResolved:', abstract)\n",
    "        \n",
    "        # Allen Anaphora Resolution\n",
    "        prediction = predictor.predict(document=abstract)  # get prediction\n",
    "        abstract = predictor.coref_resolved(abstract)  # resolved text\n",
    "        resolved_abstracts.append(abstract)\n",
    "#         print('\\nResolved:', abstract)\n",
    "        \n",
    "        title = dataframe.loc[i, 'title']\n",
    "        tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    "        \n",
    "        textRank = TextRank4Keyword()\n",
    "        textRank.analyze(abstract)\n",
    "        text_rank_scores = textRank.node_weight\n",
    "\n",
    "        # Sentence Tokenization\n",
    "        pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "        sentences = re.split(pattern, abstract)\n",
    "        sentences = [sentence for sentence in sentences if any(char.isalpha() for char in sentence)]\n",
    "\n",
    "        # N-gram scoring based on tf-idf values\n",
    "        max_score = -1\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split(' ')\n",
    "            ngrams_list = list(ngrams(words, len(title.split())))\n",
    "            score = 0\n",
    "            for ngram in ngrams_list:\n",
    "                tfidf_score = sum(tfidf_scores[vectorizer.vocabulary_.get(word.lower(), -1)] for word in ngram)\n",
    "                tr_score = sum(text_rank_scores[word] if word in text_rank_scores else 0.0 for word in ngram)\n",
    "\n",
    "#                 if ngram in list(text_rank_scores.keys()):\n",
    "#                     tr_score = (text_rank_scores[ngram])\n",
    "#                 else:\n",
    "#                     tr_score = 0.0\n",
    "                    \n",
    "                # Combine scores using weights\n",
    "                score = weights['tfidf'] * tfidf_score + weights['textrank'] * tr_score\n",
    "                if (score > max_score):\n",
    "                    max_score = score\n",
    "                    summary = ' '.join(ngram)\n",
    "                    \n",
    "                wordlist = words\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                wordlist = [w for w in wordlist if not w in stop_words]\n",
    "\n",
    "                tagged = nltk.pos_tag(wordlist)\n",
    "\n",
    "                for i in tagged:\n",
    "                    if i[1][0] == 'N' and i in wordlist:\n",
    "                        s[i[0]] += weights['nouns']\n",
    "\n",
    "        # Best n-gram is taken as title\n",
    "        summaries.append(summary)\n",
    "        # Rouge-1 score calculation\n",
    "        score = ROUGE.get_scores(summary, title)\n",
    "        p = score[\"rouge-1\"][\"p\"]\n",
    "        r = score[\"rouge-1\"][\"r\"]\n",
    "        f1 = score[\"rouge-1\"][\"f\"]\n",
    "        scores.append([p, r, f1])\n",
    "\n",
    "    dataframe['resolved abstract'] = resolved_abstracts\n",
    "    dataframe['summary'] = summaries\n",
    "    dataframe['rouge-1 score'] = scores\n",
    "    \n",
    "    return dataframe, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64578259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_weights(dataframe, weight_ranges):\n",
    "    best_weights = None\n",
    "    best_score = -1\n",
    "\n",
    "    # Generate all possible combinations of weights within the specified ranges\n",
    "    weight_combinations = list(itertools.product(*weight_ranges.values()))\n",
    "\n",
    "    for weights in weight_combinations:\n",
    "        # Set the weights for each feature\n",
    "        weight_dict = {feature: weight for feature, weight in zip(weight_ranges.keys(), weights)}\n",
    "\n",
    "        # Summarize text using the current weights\n",
    "        summaries, s = summarize_text(dataframe, weights=weight_dict)\n",
    "\n",
    "        # Evaluate the performance using an appropriate evaluation metric\n",
    "        score = evaluate_summaries(summaries,'rouge-1 score')\n",
    "\n",
    "        # Keep track of the best weights and score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = weight_dict\n",
    "\n",
    "    return best_weights, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "159d81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries(dataframe, col):\n",
    "    p_sum = 0\n",
    "    for score in dataframe[col]:\n",
    "        p_sum += score[0]\n",
    "    total = len(dataframe[col])\n",
    "    avg = p_sum/total\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe4212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ranges = {\n",
    "    'tfidf': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    'nouns': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0],\n",
    "    'textrank': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48663f4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['abstract'] = dataframe['abstract'].fillna('This abstract does not exist')\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['resolved abstract'] = resolved_abstracts\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['summary'] = summaries\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['rouge-1 score'] = scores\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['abstract'] = dataframe['abstract'].fillna('This abstract does not exist')\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['resolved abstract'] = resolved_abstracts\n",
      "C:\\Users\\srina\\AppData\\Local\\Temp\\ipykernel_23532\\3243153208.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['summary'] = summaries\n"
     ]
    }
   ],
   "source": [
    "weights = grid_search_weights(df.head(10), weight_ranges)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ad85a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf': 0.2, 'nouns': 0.1, 'textrank': 0.1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ecea80ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Microbiological Survey and Characterization</td>\n",
       "      <td>In our study, two dairy compost heaps and one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb 2 00 6 Non-Commutative Formal Groups in Pos...</td>\n",
       "      <td>We describe geometric non-commutative formal g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Alternating Mesh Quality Metric Scheme for ...</td>\n",
       "      <td>In the numerical solution of partial different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Researching Distance Learning Experiences Usin...</td>\n",
       "      <td>Qualitative case study is hardly a research te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un Motor de Transformación de Modelos con Sopo...</td>\n",
       "      <td>Resumen. En la actualidad están apareciendo un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      A Microbiological Survey and Characterization   \n",
       "1  eb 2 00 6 Non-Commutative Formal Groups in Pos...   \n",
       "2  An Alternating Mesh Quality Metric Scheme for ...   \n",
       "3  Researching Distance Learning Experiences Usin...   \n",
       "4  Un Motor de Transformación de Modelos con Sopo...   \n",
       "\n",
       "                                            abstract  \n",
       "0  In our study, two dairy compost heaps and one ...  \n",
       "1  We describe geometric non-commutative formal g...  \n",
       "2  In the numerical solution of partial different...  \n",
       "3  Qualitative case study is hardly a research te...  \n",
       "4  Resumen. En la actualidad están apareciendo un...  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/SummarizationCSTitleAbstract03.csv', lineterminator='\\n')\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df = df.head(100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "180783a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srina\\OneDrive\\Desktop\\LinuxShared\\NLP\\Text Summarization\\projectsum\\sumenv\\lib\\site-packages\\allennlp\\modules\\token_embedders\\pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
     ]
    }
   ],
   "source": [
    "df, s = summarize_text(df, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ee0ff647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = summarize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1b452838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>resolved abstract</th>\n",
       "      <th>summary</th>\n",
       "      <th>rouge-1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Microbiological Survey and Characterization</td>\n",
       "      <td>In our study, two dairy compost heaps and one ...</td>\n",
       "      <td>In our study, two dairy compost heaps and one ...</td>\n",
       "      <td>and surface locations of the</td>\n",
       "      <td>[0.2, 0.2, 0.20000000000000004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb 2 00 6 Non-Commutative Formal Groups in Pos...</td>\n",
       "      <td>We describe geometric non-commutative formal g...</td>\n",
       "      <td>We describe geometric non-commutative formal g...</td>\n",
       "      <td>the symmetric Sg and the associated graded alg...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Alternating Mesh Quality Metric Scheme for ...</td>\n",
       "      <td>In the numerical solution of partial different...</td>\n",
       "      <td>In the numerical solution of partial different...</td>\n",
       "      <td>the mesh quality within the optimization proce...</td>\n",
       "      <td>[0.18181818181818182, 0.18181818181818182, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Researching Distance Learning Experiences Usin...</td>\n",
       "      <td>Qualitative case study is hardly a research te...</td>\n",
       "      <td>Qualitative case study is hardly a research te...</td>\n",
       "      <td>an overview of the study before describing the...</td>\n",
       "      <td>[0.2, 0.21428571428571427, 0.20689655172413796]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un Motor de Transformación de Modelos con Sopo...</td>\n",
       "      <td>Resumen. En la actualidad están apareciendo un...</td>\n",
       "      <td>Resumen. En la actualidad están apareciendo un...</td>\n",
       "      <td>la arquitectura de un motor de transformación ...</td>\n",
       "      <td>[0.6428571428571429, 0.6428571428571429, 0.642...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ExoMol molecular line lists-XXVI : spectra of ...</td>\n",
       "      <td>Line lists for the sulphur-containing molecule...</td>\n",
       "      <td>Line lists for the sulphur-containing molecule...</td>\n",
       "      <td>radical) and NS are shown and comparisons made...</td>\n",
       "      <td>[0.2, 0.2, 0.20000000000000004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Multiround Private Information Retrieval: Capa...</td>\n",
       "      <td>Private information retrieval (PIR) is the pro...</td>\n",
       "      <td>Private information retrieval (PIR) is the pro...</td>\n",
       "      <td>capacity of Private information retrieval (PIR...</td>\n",
       "      <td>[0.5, 0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Efficient Integral Equation Algorithms and The...</td>\n",
       "      <td>Efficient Integral Equation Algorithms and The...</td>\n",
       "      <td>Efficient Integral Equation Algorithms and The...</td>\n",
       "      <td>of low frequency magnetic fields are discussed...</td>\n",
       "      <td>[0.1, 0.1, 0.10000000000000002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Are the Determinants of Markup Size Industry-S...</td>\n",
       "      <td>The aim of this paper is to identify factors t...</td>\n",
       "      <td>The aim of this paper is to identify factors t...</td>\n",
       "      <td>affect the pricing policy in Slovenian manufac...</td>\n",
       "      <td>[0.5384615384615384, 0.5, 0.5185185185185186]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Distributed Recovery of Node Failures in Wirel...</td>\n",
       "      <td>Wireless sensor and actor networks (WSANs) add...</td>\n",
       "      <td>Wireless sensor and actor networks (WSANs) add...</td>\n",
       "      <td>is to localize the scope of the recovery and m...</td>\n",
       "      <td>[0.2727272727272727, 0.2727272727272727, 0.272...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0       A Microbiological Survey and Characterization   \n",
       "1   eb 2 00 6 Non-Commutative Formal Groups in Pos...   \n",
       "2   An Alternating Mesh Quality Metric Scheme for ...   \n",
       "3   Researching Distance Learning Experiences Usin...   \n",
       "4   Un Motor de Transformación de Modelos con Sopo...   \n",
       "..                                                ...   \n",
       "95  ExoMol molecular line lists-XXVI : spectra of ...   \n",
       "96  Multiround Private Information Retrieval: Capa...   \n",
       "97  Efficient Integral Equation Algorithms and The...   \n",
       "98  Are the Determinants of Markup Size Industry-S...   \n",
       "99  Distributed Recovery of Node Failures in Wirel...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   In our study, two dairy compost heaps and one ...   \n",
       "1   We describe geometric non-commutative formal g...   \n",
       "2   In the numerical solution of partial different...   \n",
       "3   Qualitative case study is hardly a research te...   \n",
       "4   Resumen. En la actualidad están apareciendo un...   \n",
       "..                                                ...   \n",
       "95  Line lists for the sulphur-containing molecule...   \n",
       "96  Private information retrieval (PIR) is the pro...   \n",
       "97  Efficient Integral Equation Algorithms and The...   \n",
       "98  The aim of this paper is to identify factors t...   \n",
       "99  Wireless sensor and actor networks (WSANs) add...   \n",
       "\n",
       "                                    resolved abstract  \\\n",
       "0   In our study, two dairy compost heaps and one ...   \n",
       "1   We describe geometric non-commutative formal g...   \n",
       "2   In the numerical solution of partial different...   \n",
       "3   Qualitative case study is hardly a research te...   \n",
       "4   Resumen. En la actualidad están apareciendo un...   \n",
       "..                                                ...   \n",
       "95  Line lists for the sulphur-containing molecule...   \n",
       "96  Private information retrieval (PIR) is the pro...   \n",
       "97  Efficient Integral Equation Algorithms and The...   \n",
       "98  The aim of this paper is to identify factors t...   \n",
       "99  Wireless sensor and actor networks (WSANs) add...   \n",
       "\n",
       "                                              summary  \\\n",
       "0                        and surface locations of the   \n",
       "1   the symmetric Sg and the associated graded alg...   \n",
       "2   the mesh quality within the optimization proce...   \n",
       "3   an overview of the study before describing the...   \n",
       "4   la arquitectura de un motor de transformación ...   \n",
       "..                                                ...   \n",
       "95  radical) and NS are shown and comparisons made...   \n",
       "96  capacity of Private information retrieval (PIR...   \n",
       "97  of low frequency magnetic fields are discussed...   \n",
       "98  affect the pricing policy in Slovenian manufac...   \n",
       "99  is to localize the scope of the recovery and m...   \n",
       "\n",
       "                                        rouge-1 score  \n",
       "0                     [0.2, 0.2, 0.20000000000000004]  \n",
       "1                                     [0.0, 0.0, 0.0]  \n",
       "2   [0.18181818181818182, 0.18181818181818182, 0.1...  \n",
       "3     [0.2, 0.21428571428571427, 0.20689655172413796]  \n",
       "4   [0.6428571428571429, 0.6428571428571429, 0.642...  \n",
       "..                                                ...  \n",
       "95                    [0.2, 0.2, 0.20000000000000004]  \n",
       "96                                    [0.5, 0.5, 0.5]  \n",
       "97                    [0.1, 0.1, 0.10000000000000002]  \n",
       "98      [0.5384615384615384, 0.5, 0.5185185185185186]  \n",
       "99  [0.2727272727272727, 0.2727272727272727, 0.272...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4e6abf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the numerical solution of partial differential equations (PDEs), high-quality meshes are crucial for the stability, accuracy, and convergence of the associated PDE solver. Mesh quality improvement is often performed to improve the quality of meshes before use in numerical solution of the PDE. Mesh smoothing (performed via optimization) is one popular technique for improving the mesh quality; it does so by making adjustments to the vertex locations. When an inefficient mesh quality metric is used to design the optimization problem, and hence also to measure the mesh quality within the optimization procedure, convergence of the optimization method can be much slower than desired. However, for many applications, the choice of mesh quality metric and the optimization problem should be considered fixed. In this paper, we propose a simple mesh quality metric alternation scheme for use in the mesh optimization process. The idea is to alternate the use of the original inefficient mesh quality metric with a more efficient mesh quality metric on alternate iterations of the mesh optimization procedure in order to reduce the time to convergence, while solving the original mesh quality improvement problem. Typical results of using our application scheme to solve mesh quality improvement problems yield approximately 40-55% improvement in comparison to the original mesh optimization procedure. More frequent use of the efficient metric results in greater speed-ups.\n"
     ]
    }
   ],
   "source": [
    "print(df['abstract'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9f7ec78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mesh quality within the optimization procedure, convergence of the optimization\n"
     ]
    }
   ],
   "source": [
    "print(df['summary'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cf018ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Alternating Mesh Quality Metric Scheme for Efficient Mesh Quality Improvement\n"
     ]
    }
   ],
   "source": [
    "print(df['title'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bc8979de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26319691857849753, 0.26249436272511384, 0.2621993591475638]\n"
     ]
    }
   ],
   "source": [
    "p_sum = r_sum = f1_sum = 0\n",
    "for score in df['rouge-1 score']:\n",
    "    p_sum += score[0]\n",
    "    r_sum += score[1]\n",
    "    f1_sum += score[2]\n",
    "total = len(df['rouge-1 score'])\n",
    "avg = [p_sum/total, r_sum/total, f1_sum/total]\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0d8e9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV file\n",
    "df.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bcdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumenv",
   "language": "python",
   "name": "sumenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
